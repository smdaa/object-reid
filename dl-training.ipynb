{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95528b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "random.seed(100)\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from triplet_utils import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8b8fd",
   "metadata": {},
   "source": [
    "## Reading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf982c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"2DInstances.txt\")) as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    next(reader)\n",
    "    instances = [i for i in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b662eb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 1970\n"
     ]
    }
   ],
   "source": [
    "nb_instances = len(instances)\n",
    "print(f\"Number of training instances: {nb_instances}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0bef3",
   "metadata": {},
   "source": [
    "## Triplets Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e981540f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cb162ed463448aaf54e3c2898c36de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "triplets_possibilities = {}\n",
    "\n",
    "for idx in tqdm(range(len(instances))):\n",
    "    \n",
    "    anchor = instances[idx]\n",
    "    \n",
    "    all_possible_positives, all_possible_negatives, triplet_found = find_triplet(anchor, instances)\n",
    "\n",
    "    triplets_possibilities[str(anchor)] = {\n",
    "        \"pos\": all_possible_positives,\n",
    "        \"neg\": all_possible_negatives\n",
    "    }\n",
    "\n",
    "with open(\"saved_dict/triplets_possibilities.pkl\", \"wb\") as f:\n",
    "    pickle.dump(triplets_possibilities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba4415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saved_dict/triplets_possibilities.pkl\", \"rb\") as f:\n",
    "    triplets_possibilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043eb4c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triplets possibilities: 1970\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of triplets possibilities: {len(triplets_possibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15cfc59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02470c08f01e4f6885e227cd79a6f6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anchors = []\n",
    "positives = []\n",
    "negatives = []\n",
    "removed_idx = []\n",
    "triplet_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(instances))):\n",
    "    anchor = instances[idx]\n",
    "    pos = triplets_possibilities[str(anchor)][\"pos\"]\n",
    "    neg = triplets_possibilities[str(anchor)][\"neg\"]\n",
    "    valid, positive, negative = sample_triplets(pos, neg)\n",
    "\n",
    "    if valid:\n",
    "        # these lists are for the idx-based iteration of torch.util.dataset\n",
    "        anchors.append(anchor)\n",
    "        positives.append(positive)\n",
    "        negatives.append(negative)\n",
    "\n",
    "        # this allows an O(1) access to a triplet by its key\n",
    "        triplet_dict[str(anchor)] = {\n",
    "            \"anchor\": anchor,\n",
    "            \"pos\": positive,\n",
    "            \"neg\": negative\n",
    "        }\n",
    "    else:\n",
    "        removed_idx.append(idx)\n",
    "\n",
    "# remove all instances for which we could not find a valid triplet\n",
    "instances = [i for idx, i in enumerate(instances) if idx not in removed_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a2d193f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid triplets: 469\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of valid triplets: {len(triplet_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7679de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, anchors, positives, negatives, transform=None):\n",
    "        self.transform = transform\n",
    "        self.anchors = anchors\n",
    "        self.positives = positives\n",
    "        self.negatives = negatives\n",
    "        self.size = len(self.anchors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor = load_instance(self.anchors[idx], self.transform)\n",
    "        p = load_instance(self.positives[idx], self.transform)\n",
    "        n = [load_instance(neg, transform) for neg in self.negatives[idx]]\n",
    "\n",
    "        return {\n",
    "            \"anchor\": anchor,\n",
    "            \"pos\": p,\n",
    "            \"neg\": n\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ef40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripletDataset(anchors, positives, negatives, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382a0c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 469\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset size: {dataset.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623b5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d0978c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "validation_percentage = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_percentage * dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb981d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': {'image': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       "  'bbox': {'x': tensor([76, 53, 48, 48, 48, 48, 70, 48, 69, 48, 48, 48, 48, 48, 48, 61]),\n",
       "   'y': tensor([31,  0, 40,  0, 55, 77, 15,  0,  0, 45, 39, 22, 26, 11, 93, 18]),\n",
       "   'w': tensor([ 98,  93, 127, 127,  86, 127, 105, 108, 106, 110,  99,  68, 127,  97,\n",
       "            96, 114]),\n",
       "   'h': tensor([112,  57, 162, 164, 124,  85,  95, 112,  80, 169, 184, 133, 197,  90,\n",
       "            99, 122])},\n",
       "  'label': ['chair',\n",
       "   'shelf',\n",
       "   'stool',\n",
       "   'desk',\n",
       "   'desk',\n",
       "   'kitchen counter',\n",
       "   'chair',\n",
       "   'sofa',\n",
       "   'commode',\n",
       "   'chair',\n",
       "   'chair',\n",
       "   'chair',\n",
       "   'chair',\n",
       "   'pillow',\n",
       "   'pillow',\n",
       "   'shelf'],\n",
       "  'instance_id': tensor([ 4, 16, 11, 19, 19, 10,  5, 14,  6,  3,  4,  4,  8, 31, 28, 15]),\n",
       "  'reference': ['',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   ''],\n",
       "  'scan': ['0cac75b3-8d6f-2d13-8f4b-3d74070429df',\n",
       "   '0cac75dc-8d6f-2d13-8d08-9c497bd6acdc',\n",
       "   '0cac753a-8d6f-2d13-8f04-2073b87af440',\n",
       "   '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "   '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "   '0cac75fe-8d6f-2d13-8dd8-87c100e95f4d',\n",
       "   '0cac75b3-8d6f-2d13-8f4b-3d74070429df',\n",
       "   '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "   '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "   '0cac75fc-8d6f-2d13-8c59-13d52e366f38',\n",
       "   '0cac75b3-8d6f-2d13-8f4b-3d74070429df',\n",
       "   '0cac75b3-8d6f-2d13-8f4b-3d74070429df',\n",
       "   '0ad2d38f-79e2-2212-98d2-9b5060e5e9b5',\n",
       "   '0cac75d0-8d6f-2d13-8c26-d771a31c3f50',\n",
       "   '0a4b8ef6-a83a-21f2-8672-dce34dd0d7ca',\n",
       "   '0cac75fc-8d6f-2d13-8c59-13d52e366f38'],\n",
       "  'frame_nr': tensor([142, 108,  58, 250, 109, 132,   2, 238, 255,  96,  98,  90,  95, 123,\n",
       "          235, 105])},\n",
       " 'pos': {'image': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       "  'bbox': {'x': tensor([69, 58, 48, 71, 48, 48, 48, 48, 97, 73, 68, 48, 48, 48, 48, 76]),\n",
       "   'y': tensor([  0,  77,  36,   2,  21,  85,   8,  54, 122,  32,   0,   0,  15,  12,\n",
       "             0,  62]),\n",
       "   'w': tensor([106, 111, 107, 104,  87, 127, 127, 127,  78, 102,  91,  80, 112,  72,\n",
       "           108,  99]),\n",
       "   'h': tensor([171,  70, 147, 161, 126,  87, 133, 153, 101, 106, 110, 111, 208,  98,\n",
       "           156, 104])},\n",
       "  'label': ['chair',\n",
       "   'shelf',\n",
       "   'stool',\n",
       "   'desk',\n",
       "   'desk',\n",
       "   'kitchen counter',\n",
       "   'chair',\n",
       "   'sofa',\n",
       "   'commode',\n",
       "   'chair',\n",
       "   'chair',\n",
       "   'chair',\n",
       "   'chair',\n",
       "   'pillow',\n",
       "   'pillow',\n",
       "   'shelf'],\n",
       "  'instance_id': tensor([ 4, 16, 13, 19, 19, 10,  5, 14,  6,  3,  4,  4,  8, 31, 28, 15]),\n",
       "  'reference': ['',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   ''],\n",
       "  'scan': ['0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "   '0cac75de-8d6f-2d13-8e1a-b574569c3885',\n",
       "   '0cac753a-8d6f-2d13-8f04-2073b87af440',\n",
       "   '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "   '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "   '0cac75fe-8d6f-2d13-8dd8-87c100e95f4d',\n",
       "   '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "   '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "   '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "   '0cac75fe-8d6f-2d13-8dd8-87c100e95f4d',\n",
       "   '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "   '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "   '0ad2d391-79e2-2212-98a3-7e2440884395',\n",
       "   '0cac75d0-8d6f-2d13-8c26-d771a31c3f50',\n",
       "   '0a4b8ef6-a83a-21f2-8672-dce34dd0d7ca',\n",
       "   '0cac75fe-8d6f-2d13-8dd8-87c100e95f4d'],\n",
       "  'frame_nr': tensor([ 27,  12,  42, 249, 244, 133, 261, 195, 263,  15,  15,  16,  35, 122,\n",
       "          287,  72])},\n",
       " 'neg': [{'image': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "            [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       "   'bbox': {'x': tensor([ 94,  48,  48,  48,  48,  86,  48,  48,  92, 118,  94,  48,  80,  48,\n",
       "             48, 107]),\n",
       "    'y': tensor([ 0, 69, 75,  4,  0,  0,  0, 51, 50, 46,  0,  7, 48,  0,  0, 91]),\n",
       "    'w': tensor([ 81,  59, 127, 127, 127,  89, 127,  83,  83,  57,  81, 124,  95, 127,\n",
       "            127,  68]),\n",
       "    'h': tensor([ 89, 113, 124, 154, 148,  58, 170, 113, 173, 153,  89, 140, 175, 160,\n",
       "            223, 132])},\n",
       "   'label': ['chair',\n",
       "    'shelf',\n",
       "    'counter',\n",
       "    'sofa',\n",
       "    'sofa',\n",
       "    'kitchen cabinet',\n",
       "    'chair',\n",
       "    'side table',\n",
       "    'chair',\n",
       "    'chair',\n",
       "    'chair',\n",
       "    'chair',\n",
       "    'wardrobe',\n",
       "    'couch',\n",
       "    'bed',\n",
       "    'shelf'],\n",
       "   'instance_id': tensor([17, 47,  4, 14, 14, 14,  4, 16,  5,  2, 17, 17,  7,  2, 17, 17]),\n",
       "   'reference': ['',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    '',\n",
       "    ''],\n",
       "   'scan': ['0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "    '0cac75de-8d6f-2d13-8e1a-b574569c3885',\n",
       "    '0cac753a-8d6f-2d13-8f04-2073b87af440',\n",
       "    '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "    '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "    '0cac75fe-8d6f-2d13-8dd8-87c100e95f4d',\n",
       "    '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "    '0cac75b7-8d6f-2d13-8cb2-0b4e06913140',\n",
       "    '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "    '0cac75fe-8d6f-2d13-8dd8-87c100e95f4d',\n",
       "    '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "    '0cac75b1-8d6f-2d13-8c17-9099db8915bc',\n",
       "    '0ad2d38f-79e2-2212-98d2-9b5060e5e9b5',\n",
       "    '0cac75d0-8d6f-2d13-8c26-d771a31c3f50',\n",
       "    '0a4b8ef6-a83a-21f2-8672-dce34dd0d7ca',\n",
       "    '0cac75fe-8d6f-2d13-8dd8-87c100e95f4d'],\n",
       "   'frame_nr': tensor([165,   2,   6, 104, 113,  27,  26, 230, 276,  36, 165, 187, 171,  42,\n",
       "           117,  42])}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32543fbe",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d3c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0734283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(MyResNet, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        self.start = torch.nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool\n",
    "        )\n",
    "\n",
    "        self.slice1 = resnet.layer1\n",
    "        self.slice2 = resnet.layer2\n",
    "        self.slice3 = resnet.layer3\n",
    "        self.slice4 = resnet.layer4\n",
    "        \n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.start(X)\n",
    "        h = self.slice1(h)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        return [h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4e8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, add_conv=True):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.add_conv = add_conv\n",
    "        self.model = MyResNet(requires_grad=requires_grad)\n",
    "        if self.add_conv:\n",
    "            self.conv1 = torch.nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=7)\n",
    "            self.max1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "            self.conv2 = torch.nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=4)\n",
    "            self.max2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "            self.conv3 = torch.nn.Conv2d(in_channels=2048, out_channels=2048, kernel_size=2)\n",
    "            self.max3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "            self.flat = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch[\"image\"]\n",
    "        x = self.model(x)[1]\n",
    "        if self.add_conv:\n",
    "            x = self.conv1(x)\n",
    "            x = self.max1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.max2(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.max3(x)\n",
    "            x = self.flat(x)\n",
    "        \n",
    "        return x # return list of size 1 to have same API as the other encoders that return list of mulitple encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab8aead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b0231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f1543f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of MyModel(\n",
       "  (model): MyResNet(\n",
       "    (start): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (slice1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (slice2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (slice3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (slice4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv2d(512, 1024, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (max1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (max2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(2048, 2048, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (max3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       ")>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aff59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "conv3.weight\n",
      "conv3.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b632057",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51085e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_dist_sum_weighted_per_batch(input_features, target_features):\n",
    "    \"\"\"\n",
    "    Calculates l2 distance between input and target features and weights distance by number of feature maps.\n",
    "    :param input_features: list of feature-maps\n",
    "    :param target_features: list of feature-maps\n",
    "    :param reduction: see nn.MSELoss --> this controls if we reduce all batches at the end by summing/averaging or not.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_dists = []\n",
    "    for i in range(input_features[0].shape[0]):\n",
    "        dists = []\n",
    "        for input, target in zip(input_features, target_features):\n",
    "            dists.append(nn.MSELoss()(input[i], target[i]))\n",
    "        batch_dists.append(torch.mean(torch.stack(dists)))\n",
    "    return torch.stack(batch_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70ba4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, margin=0.5, reduction_mode=\"mean\"):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.reduction_mode = reduction_mode\n",
    "\n",
    "    def forward(self, encodings):\n",
    "\n",
    "        # get anchor, pos, negs from encodings\n",
    "        anchor = [encodings[i][\"anchor\"] for i in range(len(encodings))]\n",
    "        pos = [encodings[i][\"pos\"] for i in range(len(encodings))]\n",
    "        negs = [encodings[i][\"neg\"] for i in range(len(encodings))]\n",
    "        if len(negs[0]) != 1:\n",
    "            raise ValueError(f\"Only one negative is allowed for TripletLoss, but got: {len(negs[0])}\")\n",
    "        negs = [negs[k][0] for k in range(len(negs))]\n",
    "\n",
    "        # calculate anchor->pos dist\n",
    "        pos_dist = l2_dist_sum_weighted_per_batch(anchor, pos)\n",
    "\n",
    "        # calculate anchor->neg dist for all negs\n",
    "        neg_dist = l2_dist_sum_weighted_per_batch(anchor, negs)\n",
    "\n",
    "        losses = F.relu(pos_dist - neg_dist + self.margin)\n",
    "\n",
    "        # calculate triplet loss\n",
    "        if self.reduction_mode == \"mean\":\n",
    "            return F.relu(pos_dist - neg_dist + self.margin).mean(), pos_dist.mean().data.cpu().numpy(), neg_dist.mean().data.cpu().numpy()\n",
    "        elif self.reduction_mode == \"sum\":\n",
    "            return F.relu(pos_dist - neg_dist + self.margin).sum(), pos_dist.sum().data.cpu().numpy(), neg_dist.sum().data.cpu().numpy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported reduction_mode:{self.reduction_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc5181d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=TripletLoss(margin=0.1, reduction_mode=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f02cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam\n",
    "scheduler=torch.optim.lr_scheduler.StepLR\n",
    "optim_args={\n",
    "    \"lr\": 1e-5,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": 0.0\n",
    "}\n",
    "scheduler_args={\n",
    "    \"step_size\": 15,\n",
    "    \"gamma\": 0.1,\n",
    "}\n",
    "top_k_accs = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13e99722",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optim(filter(lambda p: p.requires_grad, model.parameters()), **optim_args)\n",
    "scheduler = scheduler(optim, **scheduler_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "589d0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, sample, device):\n",
    "    sample = triplets_as_batches(sample, 1)\n",
    "    if torch.cuda.is_available():\n",
    "        sample[\"image\"] = sample[\"image\"].to(device)\n",
    "    encodings = model(sample)\n",
    "    encodings = outputs_as_triplets(encodings, 1)\n",
    "    loss, pos_dist, neg_dist = loss_func(encodings)\n",
    "\n",
    "    return loss, pos_dist, neg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7184b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optim, epoch, iter_per_epoch, device, \n",
    "                    log_nth_iter, log_nth_epoch, num_epochs):\n",
    "    model.train()  # TRAINING mode (for dropout, batchnorm, etc.)\n",
    "    train_losses = []\n",
    "    train_pos_dists = []\n",
    "    train_neg_dists = []\n",
    "\n",
    "    train_minibatches = train_loader\n",
    "    train_minibatches = tqdm(train_minibatches)\n",
    "\n",
    "    for i, sample in enumerate(train_minibatches):  # for every minibatch in training set\n",
    "\n",
    "        # FORWARD PASS --> Loss + acc calculation\n",
    "        train_loss, train_pos_dist, train_neg_dist = forward_pass(model, sample, device)\n",
    "        \n",
    "        # BACKWARD PASS --> Gradient-Descent update\n",
    "        train_loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # LOGGING of loss and accuracy\n",
    "        train_loss = train_loss.data.cpu().numpy()\n",
    "        train_losses.append(train_loss)\n",
    "        train_pos_dists.append(train_pos_dist)\n",
    "        train_neg_dists.append(train_neg_dist)\n",
    "\n",
    "        # Print loss every log_nth iteration\n",
    "        if log_nth_iter != 0 and (i + 1) % log_nth_iter == 0:\n",
    "            print(\"[Iteration {cur}/{max}] TRAIN loss: {loss}\".format(cur=i + 1,\n",
    "                                                                          max=iter_per_epoch,\n",
    "                                                                          loss=train_loss))\n",
    "\n",
    "    # ONE EPOCH PASSED --> calculate + log mean train accuracy/loss for this epoch\n",
    "    mean_train_loss = np.mean(train_losses)\n",
    "    mean_train_pos_dist = np.mean(train_pos_dists)\n",
    "    mean_train_neg_dist = np.mean(train_neg_dists)\n",
    "\n",
    "    if log_nth_epoch != 0 and (epoch + 1) % log_nth_epoch == 0:\n",
    "        print(\"[EPOCH {cur}/{max}] TRAIN mean loss / pos_dist / neg_dist: {loss}, {pos_dist}, {neg_dist}\".format(\n",
    "            cur=epoch + 1,\n",
    "            max=num_epochs,\n",
    "            loss=mean_train_loss,\n",
    "            pos_dist=mean_train_pos_dist,\n",
    "            neg_dist=mean_train_neg_dist))\n",
    "\n",
    "    return mean_train_loss, mean_train_pos_dist, mean_train_neg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4c2fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(model, val_loader, epoch, device, log_nth_iter, log_nth_epoch, num_epochs):\n",
    "    # ONE EPOCH PASSED --> calculate + log validation accuracy/loss for this epoch\n",
    "    model.eval()  # EVAL mode (for dropout, batchnorm, etc.)\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        val_pos_dists = []\n",
    "        val_neg_dists = []\n",
    "\n",
    "        val_minibatches = val_loader\n",
    "        val_minibatches = tqdm(val_minibatches)\n",
    "\n",
    "        for i, sample in enumerate(val_minibatches):\n",
    "            # FORWARD PASS --> Loss + acc calculation\n",
    "            val_loss, val_pos_dist, val_neg_dist = forward_pass(model, sample, device)\n",
    "\n",
    "            # LOGGING of loss and accuracy\n",
    "            val_loss = val_loss.data.cpu().numpy()\n",
    "            val_losses.append(val_loss)\n",
    "            val_pos_dists.append(val_pos_dist)\n",
    "            val_neg_dists.append(val_neg_dist)\n",
    "\n",
    "            # Print loss every log_nth iteration\n",
    "            if log_nth_iter != 0 and (i + 1) % log_nth_iter == 0:\n",
    "                    print(\"[Iteration {cur}/{max}] Val loss: {loss}\".format(cur=i + 1,\n",
    "                                                                            max=len(val_loader),\n",
    "                                                                            loss=val_loss))\n",
    "\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        mean_val_pos_dist = np.mean(val_pos_dists)\n",
    "        mean_val_neg_dist = np.mean(val_neg_dists)\n",
    "\n",
    "        if log_nth_epoch != 0 and (epoch + 1) % log_nth_epoch == 0:\n",
    "            print(\"[EPOCH {cur}/{max}] VAL mean loss / pos_dist / neg_dist: {loss}, {pos_dist}, {neg_dist}\".format(\n",
    "                    cur=epoch + 1,\n",
    "                    max=num_epochs,\n",
    "                    loss=mean_val_loss,\n",
    "                    pos_dist=mean_val_pos_dist,\n",
    "                    neg_dist=mean_val_neg_dist))\n",
    "\n",
    "        return mean_val_loss, mean_val_pos_dist, mean_val_neg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4edc5963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, start_epoch=0, num_epochs=10, \n",
    "          log_nth_iter=1, log_nth_epoch=1):\n",
    "        \"\"\"\n",
    "        Train a given model with the provided data.\n",
    "        Inputs:\n",
    "        - model: model object initialized from a torch.nn.Module\n",
    "        - train_loader: train data in torch.utils.data.DataLoader\n",
    "        - val_loader: val data in torch.utils.data.DataLoader\n",
    "        - num_epochs: total number of training epochs\n",
    "        - log_nth: log training accuracy and loss every nth iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # model to cuda before optim creation: https://pytorch.org/docs/stable/optim.html\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # init loss / acc history and train length\n",
    "        iter_per_epoch = len(train_loader)\n",
    "\n",
    "        # start training\n",
    "        print('START TRAIN on device: {}'.format(device))\n",
    "\n",
    "        max_epoch = start_epoch + num_epochs\n",
    "        epochs = range(start_epoch, max_epoch)\n",
    "        epochs = tqdm(range(start_epoch, max_epoch))\n",
    "\n",
    "        # epoch loop\n",
    "        for epoch in epochs:\n",
    "\n",
    "            # train iterations for one epoch\n",
    "            mean_train_loss, mean_train_pos_dist, mean_train_neg_dist = train_one_epoch(model,\n",
    "                                                                                        train_loader,\n",
    "                                                                                        optim,\n",
    "                                                                                        epoch,\n",
    "                                                                                        iter_per_epoch,\n",
    "                                                                                        device,\n",
    "                                                                                        log_nth_iter,\n",
    "                                                                                        log_nth_epoch,\n",
    "                                                                                        num_epochs)\n",
    "\n",
    "            # val iterations for one epoch\n",
    "            mean_val_loss, mean_val_pos_dist, mean_val_neg_dist = val_one_epoch(model,\n",
    "                                                                                val_loader,\n",
    "                                                                                epoch,\n",
    "                                                                                device,\n",
    "                                                                                log_nth_iter,\n",
    "                                                                                log_nth_epoch,\n",
    "                                                                                num_epochs)\n",
    "\n",
    "            # Decay Learning Rate\n",
    "            # Currently, only the ReduceLROnPlateau scheduler needs an argument (last val loss).\n",
    "            # ALl others are a non-argument call to step() method.\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(self.val_loss_history[-1])\n",
    "            else:\n",
    "                scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a0b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN on device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b388b1042043769d4d16855e335b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44916bcf853d4049833748242901b25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1/20] TRAIN mean loss / pos_dist / neg_dist: 0.09514176100492477, 0.0039747622795403, 0.010540000163018703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d9e2a6f6364dc3bd6016d17d5b80cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1/20] VAL mean loss / pos_dist / neg_dist: 0.08688118308782578, 0.016223536804318428, 0.03737553954124451\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0c19b4800e481da09c1f793efbe185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 2/20] TRAIN mean loss / pos_dist / neg_dist: 0.06816696375608444, 0.04619154334068298, 0.1385985165834427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1911f3d9f96b47f8a6f3376f450c67c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 2/20] VAL mean loss / pos_dist / neg_dist: 0.08459172397851944, 0.05373213067650795, 0.14346247911453247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6f14e4f83547499f6814eaa3466715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 3/20] TRAIN mean loss / pos_dist / neg_dist: 0.05093863233923912, 0.050353217869997025, 0.20076493918895721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e16ee1f2ef045f380ff1fcbbbfb5448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 3/20] VAL mean loss / pos_dist / neg_dist: 0.08247614651918411, 0.06875953078269958, 0.1957845538854599\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fb60e2b503468da3cc143b41ff69d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 4/20] TRAIN mean loss / pos_dist / neg_dist: 0.03523515164852142, 0.06831726431846619, 0.31920644640922546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae285ad71f684a86a873c254ffce2c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, num_epochs=20, log_nth_iter=0, log_nth_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe938dea",
   "metadata": {},
   "source": [
    "## Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33539bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(modelname, model):\n",
    "    from pathlib import Path\n",
    "    path = \"./saved_models\"\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    # Might need to make sure, that the correct saved_results directory is chosen here.\n",
    "    filepath = path + \"/\" + modelname + \".pt\"\n",
    "    torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique ID for this training process for saving to disk.\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "now = datetime.now() # current date and time\n",
    "id_suffix = now.strftime(\"%Y-%b-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"model_\" + id_suffix\n",
    "save_model(modelname, model)\n",
    "print(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6cc39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
